{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Selection\n",
    "**_BERT_** is a **_Multi-layer_ _Multi-Head_** Transformer architecture. As discuss in many of the current reseachers, different Attention heads captures different lingustic patterns. For a better deletion of words using Attention mechanism we need to choose a head which **captures pattern useful for classification.**\n",
    "\n",
    "To do this we are using a Brute force mechanism to seach through all the possible heads. We are deleting TopK words attended by different heads from the sentence and measuring the new classification score. In case of sentiments, removing sentiments related words makes the sentence neutral. The heads are sorted by the amount to which it is able to make the sentences from dev set to Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "#from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "\n",
    "from bertviz.bertviz import attention, visualization\n",
    "from bertviz.bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2020 23:29:28 - INFO - __main__ -   device: cuda, n_gpu 4\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#TDRG paper\n",
    "#bert_classifier_model_dir = \"./bert_classifier/\" ## Path of BERT classifier model path\n",
    "\n",
    "bert_classifier_model_dir = './data/yelp/bert_classifier_3epochs/'\n",
    "\n",
    "# Lipton\n",
    "#bert_classifier_model_dir = \"./data/lipton/sentiment/orig/bert_classifier_10epochs8b_490seqlen/\"\n",
    "#eval_accuracy = 0.9102040816326531  and  eval_loss = 0.35673839559838655  \n",
    "\n",
    "bert_classifier_model_dir = \"./data/lipton/sentiment/orig/bert_classifier_100epochs8b_490seqlen/\"  #Apr 24\n",
    "#eval_accuracy = 0.8979591836734694   and eval_loss = 0.9967757850885384           # Try with this one <--\n",
    "\n",
    "\n",
    "# Image Caption\n",
    "bert_classifier_model_dir = \"./data/imagecaption/bert_classifier_10epochs/\"\n",
    "\n",
    "#Amazon \n",
    "bert_classifier_model_dir = \"./data/amazon/bert_classifier_3epochs/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n",
    "\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2020 23:29:31 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ./data/amazon/bert_classifier_3epochs/\n",
      "05/16/2020 23:29:31 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/16/2020 23:29:34 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/diego/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model for performing Classification\n",
    "model_cls = BertForSequenceClassification.from_pretrained(bert_classifier_model_dir, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model_cls.to(device)\n",
    "model_cls.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/16/2020 23:29:44 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   loading archive file ./data/amazon/bert_classifier_3epochs/\n",
      "05/16/2020 23:29:44 - INFO - bertviz.bertviz.pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/16/2020 23:29:45 - INFO - bertviz.bertviz.pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/diego/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model to get the attention weights of all the heads\n",
    "model = BertModel.from_pretrained(bert_classifier_model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=70 # Maximum sequence length  for TDRG / IMAGE CAPTION / AMAZON\n",
    "\n",
    "#max_seq_len=490   # for Lipton Lipton\n",
    "sm = torch.nn.Softmax(dim=-1) ## Softmax over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_examples(input_sentences, bs=32):\n",
    "    \"\"\"\n",
    "    This fucntion returns classification predictions for batch of sentences.\n",
    "    input_sentences: list of strings\n",
    "    bs : batch_size : int\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Prepare data for classification\n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    pred_lt = []\n",
    "    for sen in input_sentences:\n",
    "        text_tokens = tokenizer.tokenize(sen)\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "\n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    ## Convert input lists to Torch Tensors\n",
    "    ids = torch.tensor(ids)\n",
    "    segment_ids = torch.tensor(segment_ids)\n",
    "    input_masks = torch.tensor(input_masks)\n",
    "    \n",
    "    steps = len(ids) // bs\n",
    "    \n",
    "    for i in range(steps+1):\n",
    "        if i == steps:\n",
    "            temp_ids = ids[i * bs : len(ids)]\n",
    "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
    "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
    "        else:\n",
    "            temp_ids = ids[i * bs : i * bs + bs]\n",
    "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
    "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
    "        \n",
    "        temp_ids = temp_ids.to(device)\n",
    "        temp_segment_ids = temp_segment_ids.to(device)\n",
    "        temp_input_masks = temp_input_masks.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = sm(model_cls(temp_ids, temp_segment_ids, temp_input_masks))\n",
    "        pred_lt.extend(preds.tolist())\n",
    "    \n",
    "    return pred_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,size):\n",
    "    with open(path) as fp:\n",
    "        data = fp.read().splitlines()[:size]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_for_batch(input_sentences, bs=32):\n",
    "    \"\"\"\n",
    "    This function calculates attention weights of all the heads and\n",
    "    returns it along with the encoded sentence for further processing.\n",
    "    \n",
    "    input sentence: list of strings\n",
    "    bs : batch_size\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Preprocessing for BERT \n",
    "    ids = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    pred_lt = []\n",
    "    ids_for_decoding = []\n",
    "    for sen in input_sentences:\n",
    "        text_tokens = tokenizer.tokenize(sen)\n",
    "        tokens = [\"[CLS]\"] + text_tokens + [\"[SEP]\"]\n",
    "        temp_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        input_mask = [1] * len(temp_ids)\n",
    "        segment_id = [0] * len(temp_ids)\n",
    "        padding = [0] * (max_seq_len - len(temp_ids))\n",
    "        \n",
    "        ids_for_decoding.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "        temp_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_id += padding\n",
    "        \n",
    "        ids.append(temp_ids)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        \n",
    "    ## Convert the list of int ids to Torch Tensors\n",
    "    ids = torch.tensor(ids)\n",
    "    segment_ids = torch.tensor(segment_ids)\n",
    "    input_masks = torch.tensor(input_masks)\n",
    "    \n",
    "    steps = len(ids) // bs\n",
    "    \n",
    "    for i in trange(steps+1):\n",
    "        if i == steps:\n",
    "            temp_ids = ids[i * bs : len(ids)]\n",
    "            temp_segment_ids = segment_ids[i * bs: len(ids)]\n",
    "            temp_input_masks = input_masks[i * bs: len(ids)]\n",
    "        else:\n",
    "            temp_ids = ids[i * bs : i * bs + bs]\n",
    "            temp_segment_ids = segment_ids[i * bs: i * bs + bs]\n",
    "            temp_input_masks = input_masks[i * bs: i * bs + bs]\n",
    "        \n",
    "        temp_ids = temp_ids.to(device)\n",
    "        temp_segment_ids = temp_segment_ids.to(device)\n",
    "        temp_input_masks = temp_input_masks.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, _, attn = model(temp_ids, temp_segment_ids, temp_input_masks)\n",
    "        \n",
    "        # Add all the Attention Weights to CPU memory\n",
    "        # Attention weights for each layer is stored in a dict 'attn_prob'\n",
    "        for k in range(12):\n",
    "            attn[k]['attn_probs'] = attn[k]['attn_probs'].to('cpu')\n",
    "        \n",
    "        '''\n",
    "        attention weights are stored in this way:\n",
    "        att_lt[layer]['attn_probs']['input_sentence']['head']['length_of_sentence']\n",
    "        '''\n",
    "        # Concate Attention weights for all the examples in the list att_lt[layer_no]['attn_probs']\n",
    "        \n",
    "        if i == 0:\n",
    "            att_lt = attn\n",
    "            heads = len(att_lt)\n",
    "        else:\n",
    "            for j in range(heads):\n",
    "                att_lt[j]['attn_probs'] = torch.cat((att_lt[j]['attn_probs'],attn[j]['attn_probs']),0)\n",
    "        \n",
    "    \n",
    "    return att_lt, ids_for_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(input_sentences, att, decoding_ids, threshold=0.25):\n",
    "    \"\"\"\n",
    "    This function processes each input sentence by removing the top tokens defined threshold value.\n",
    "    Each sentence is processed for each head.\n",
    "    \n",
    "    input_ids: list of strings\n",
    "    decoding_ids: indexed input_sentnces thus len(input_sentences) == len(decoding_ids)\n",
    "    threshold: Percentage of the top indexes to be removed\n",
    "    \"\"\"\n",
    "    # List of None of num_of_layers * num_of_heads to save the results of each head for input_sentences\n",
    "    \n",
    "    lt = [None for x in range(len(att) * len(att[0]['attn_probs'][0]))]\n",
    "    #print(len(lt))\n",
    "    \n",
    "    inx = 0\n",
    "    for i in trange(len(att)): #  For all the layers\n",
    "        for j in range(len(att[i]['attn_probs'][0])): # For all the heads in the ith Layer\n",
    "            processed_sen = [None for q in decoding_ids] # List of len(decoding_ids)\n",
    "            for k in range(len(input_sentences)): # For all the senteces \n",
    "                _, topi = att[i]['attn_probs'][k][j][0].topk(len(decoding_ids[k])) # Get top attended ids\n",
    "                topi = topi.tolist()\n",
    "                topi = topi[:int(len(topi) * threshold)] \n",
    "                ## Decode the sentece after removing the topk indexes\n",
    "                final_indexes = []\n",
    "                count = 0\n",
    "                count1 = 0\n",
    "                tokens = [\"[CLS]\"] + tokenizer.tokenize(input_sentences[k]) + [\"[SEP]\"]\n",
    "                while count < len(decoding_ids[k]):\n",
    "                    if count in topi: # Remove index if present in topk\n",
    "                        while (count + count1 + 1) < len(decoding_ids[k]):\n",
    "                            if \"##\" in tokens[count + count1 + 1]:\n",
    "                                count1 += 1\n",
    "                            else:\n",
    "                                break\n",
    "                        count += count1\n",
    "                        count1 = 0\n",
    "                    else: # Else add to the decoded sentence\n",
    "                        final_indexes.append(decoding_ids[k][count])\n",
    "                    count += 1\n",
    "                tmp = tokenizer.convert_ids_to_tokens(final_indexes) # Convert ids to token\n",
    "                # Convert toknes to sentence\n",
    "                processed_sen[k] = \" \".join(tmp).replace(\" ##\", \"\").replace(\"[CLS]\",\"\").replace(\"[SEP]\",\"\").strip()\n",
    "            lt[inx] = processed_sen # Store sentences for inxth head\n",
    "            inx += 1\n",
    "    \n",
    "    return lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_head(processed_sentence_list, lmbd = 0.1):\n",
    "    \"\"\"\n",
    "    This function calculate classification scores for sentences generated by each head\n",
    "    and sort them from best to worst.\n",
    "    score = min(pred) + lmbd / max(pred) + lmbd, lmbd is smoothing param\n",
    "    pred is list of probability score for each class, for best case pred = [0.5, 0.5] ==> score = 1\n",
    "    \n",
    "    it returns sorted list of (Layer, Head, Score)\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    #scores_1 = {}\n",
    "    for i in trange(len(processed_sentence_list)): # sentences by each head\n",
    "        pred = np.array(run_multiple_examples(processed_sentence_list[i]))\n",
    "        scores[i] = np.mean([(min(x[0], x[1])+lmbd)/(max(x[0], x[1])+lmbd) for x in pred])\n",
    "        #scores_1[i] = np.mean([abs(max(x[0],x[1]) - min(x[0],x[1])) for x in pred])\n",
    "    temp = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    #temp1 = sorted(scores_1.items(), key=lambda kv: kv[1], reverse=False)\n",
    "    score_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp]\n",
    "    #score1_lt = [(x // 12, x - (12 * (x // 12)),y) for x,y in temp1]\n",
    "    return score_lt  #score1_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDRG\n",
    "#pos_examples_file = \"/home/ubuntu/bhargav/data/yelp/sentiment_dev_1.txt\"\n",
    "#neg_examples_file = \"/home/ubuntu/bhargav/data/yelp/sentiment_dev_0.txt\"\n",
    "\n",
    "#YELP\n",
    "pos_examples_file = \"./data/yelp/sentiment_dev_1.txt\"\n",
    "neg_examples_file = \"./data/yelp/sentiment_dev_0.txt\"\n",
    "\n",
    "#IMAGE CAPTION\n",
    "pos_examples_file = \"./data/imagecaption/sentiment_dev_1.txt\"\n",
    "neg_examples_file = \"./data/imagecaption/sentiment_dev_0.txt\"\n",
    "\n",
    "#Amazon\n",
    "pos_examples_file = \"./data/amazon/sentiment_dev_1.txt\"\n",
    "neg_examples_file = \"./data/amazon/sentiment_dev_0.txt\"\n",
    "\n",
    "\n",
    "'''\n",
    "100 examples from each class worked good, the bottlenack is the run_multiple_examples() function,\n",
    "with higher memory (either with cpu of gpu) one can reduce the processing time by incresing batch_size.\n",
    "With batch_size of 32 it takes around 24 mins for 100 example on cpu.\n",
    "'''\n",
    "\n",
    "pos_data = read_file(pos_examples_file,100)\n",
    "neg_data = read_file(neg_examples_file,100)\n",
    "data = pos_data + neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for LIPTON\n",
    "#get 100 pos and 100 neg examples at random from dev file\n",
    "import pandas as pd\n",
    "datadir = \"data/lipton/sentiment/orig/\"\n",
    "devfile = os.path.join(datadir,\"dev.tsv\")\n",
    "dev = pd.read_table(devfile,sep=\"\\t\")\n",
    "pos_data, neg_data = [], []\n",
    "inds = [i for i in range(dev.shape[0])]\n",
    "random.shuffle(inds)\n",
    "for i in inds:\n",
    "    line = dev.Text.values[i]\n",
    "    cur_label = 0 if dev.Sentiment.values[i] == \"Negative\" else 1\n",
    "    if cur_label == 0:\n",
    "        if len(neg_data) < 100:\n",
    "            neg_data.append(line)\n",
    "    else:\n",
    "        if len(pos_data) < 100:\n",
    "            pos_data.append(line)\n",
    "            \n",
    "    if len(neg_data) == 100 and len(pos_data) == 100:\n",
    "        break\n",
    "data = pos_data + neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 200\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_data), len(neg_data), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 13/13 [00:01<00:00, 10.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# they implicitly use batch_size of 32 ( whereas we trained with half that)\n",
    "#att, decoding_ids = get_attention_for_batch(data)       #tdrg yelp\n",
    "\n",
    "#att, decoding_ids = get_attention_for_batch(data)       #imagecaption \n",
    "\n",
    "att, decoding_ids = get_attention_for_batch(data, 16)  #lipton / amazon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:06<00:00,  1.89it/s]\n",
      "100%|| 144/144 [01:10<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# they use a threshhold of .25%   \n",
    "# threshold: Percentage of the top indexes to be removed   <--- tHIS MIGHT BE TOO HIGH\n",
    "\n",
    "# FOR YELP/ IMAGE CAPTION / AMAZON\n",
    "\n",
    "sen_list = process_sentences(data, att, decoding_ids,threshold=0.25)  #yelp and image caption\n",
    "scores = get_block_head(sen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [01:16<00:00,  6.38s/it]\n",
      "100%|| 12/12 [01:17<00:00,  6.42s/it]\n",
      "100%|| 12/12 [01:18<00:00,  6.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# they use a threshhold of .25%   \n",
    "# threshold: Percentage of the top indexes to be removed   <--- tHIS MIGHT BE TOO HIGH\n",
    "\n",
    "#FOR LIPTON\n",
    "\n",
    "#sen_list = process_sentences(data, att, decoding_ids,threshold=0.25)  #yelp\n",
    "\n",
    "#sen_list_10 = process_sentences(data, att, decoding_ids,threshold=0.10)\n",
    "sen_list_15 = process_sentences(data, att, decoding_ids,threshold=0.15)\n",
    "sen_list_20 = process_sentences(data, att, decoding_ids,threshold=0.20)\n",
    "sen_list_30 = process_sentences(data, att, decoding_ids,threshold=0.30)\n",
    "\n",
    "#try with different thresholds !  .1, .15, .2, .25, .3      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 200\n",
      "Original\n",
      "1069 First of all, the reason I'm giving this film 2 stars instead of 1 is because at least Peter Falk gave his usual fantastic performance as Lieutenant Columbo. He alone can get 10 stars for trying to save this otherwise utterly worthless attempt at making a movie.<br /><br />I was initially all fired up at reading one poster's comment that Andrew Stevens in this movie gave \"the performance of his career.\" To me, it was the abysmal performance by Stevens that absolutely ruined this movie, and so I was all prepared to hurl all sorts of insults at the person who made the aforementioned comment. Then I thought to myself, what else has Stevens done? So I checked and, you know, that person was absolutely right. In the 17 years since this Columbo movie was made, apparently every one of the 33 projects that Stevens has been in since then has been utter crap, so it is doubtful that anybody has even seen the rest of his career.<br /><br />If you like Columbo, see every other of the 69 titles before watching this one. Do yourself a favor and save the worst for last.\n",
      "\n",
      "Threshold 10\n",
      "975 first of all , the reason i ' m giving this film 2 stars instead of 1 is because at least peter falk gave his usual performance as lieutenant columbo . he alone can get 10 stars for trying to save attempt at making a movie . < br / > < br / > i was initially all fired up at reading one poster ' s comment that andrew stevens in this movie \" the performance of his career . \" to me , it was the performance by stevens that this movie , and i all prepared to hurl all sorts of insults at the person who made the aforementioned comment . then i thought to myself , what else has stevens done ? so i checked and , you know , that person . in the 17 years since this columb movie was made , apparently every one of the 33 projects that stevens has been in since then has been utter crap , so it is doubtful that anybody has even seen the rest of his career . < br / > < br / > if you like columbo , see every other of the 69 titles before watching this one . a favor the for last\n",
      "\n",
      " Threshold 15\n",
      "928 first of all , the reason i ' m giving this film 2 stars instead of 1 is because at least peter falk gave his usual performance as lieutenant columbo . he alone can get 10 stars for trying to save attempt at making a movie . < br / > < br / > i was initially all fired up at reading one poster ' s comment that andrew in this movie \" the performance of his career . \" to it was performance by stevens that movie all prepared to hurl all sorts of insults at the person who made the aforementioned comment . then i thought to myself , what else has stevens done ? so i checked and , you know , person . in the 17 years since movie was made , apparently every one of the 33 projects that stevens has been in since then has been utter crap , so it is doubtful that anybody has even seen the rest of his career . < br / > < br / > if you like columbo , see every other of the 69 titles before watching this one . a favor the for last\n",
      "\n",
      " Threshold 20\n",
      "857 first of all , the reason i ' m giving this film 2 stars instead of 1 is because at least peter falk gave his usual performance as lieutenant columbo . he alone can get 10 stars for trying to save attempt at making a movie . < br / > < br / > i was all fired up at reading one poster ' s comment that in this movie \" performance of his career . \" to it was performance by movie all to hu all sorts of insults at the person who made the aforementioned comment . then i thought to myself , what else has stevens done ? so i checked and , you know , . in the 17 years since movie was made , apparently every one of the 33 projects that stevens has been in since then has been crap , so it is doubtful that anybody has even seen the rest of his career . < br / > < br / > if you like columbo , see every other of the 69 titles before watching one a the for last\n",
      "\n",
      " Threshold 25\n",
      "798 first of all , the reason i ' m giving this film 2 stars instead of 1 is because at least peter falk gave his usual performance as lieutenant columbo . he alone can get 10 stars for trying to save at making a movie . < br / > < br / > all fired up at reading one poster ' s comment that in movie \" performance of his career \" to it performance by movie all to hu all sorts of at the person who made the aforementioned comment . then i thought to myself , what else has stevens done ? so i and , you know . in the 17 years since movie was made , apparently every one of the 33 projects that stevens has been in since then has been , so it is doubtful that anybody has even seen the rest of his career < br / > < br / > if you like columbo , see every other of the 69 titles before watching one a for\n",
      "\n",
      " Threshold 30\n",
      "743 first of all , the i ' m giving film 2 stars instead of 1 is because at least peter falk gave his usual performance as lieutenant columbo . he alone can get 10 stars for trying to save at making a movie < br / > < br / > all up at reading one poster ' s comment that in performance of his \" to it performance by all to hu all sorts of at the person who made the aforementioned comment . then i thought to myself , what else has stevens done ? so i and , you know . in the 17 years since was made , apparently every one of the 33 projects that stevens has been in since then has been , so it is doubtful that anybody has even seen the rest of his career < br / > < br / > if you like columbo , see every other of the 69 titles before watching a\n"
     ]
    }
   ],
   "source": [
    "#List num_of_layers * num_of_heads of each layer/head of input_sentences with top 25% attributes removed\n",
    "print(len(sen_list),len(sen_list[0]))     # 12 x 12  x  200 sentences\n",
    "i = 100\n",
    "print(\"Original\")\n",
    "print(len(data[i]), data[i])           #original\n",
    "print(\"\\nThreshold 10\")\n",
    "print(len(sen_list_10[i][i]),sen_list_10[i][i])   #processed first layer and first head of first sentence \n",
    "print(\"\\n Threshold 15\")\n",
    "print(len(sen_list_15[i][i]),sen_list_15[i][i])   #processed first layer and first head of first sentence \n",
    "print(\"\\n Threshold 20\")\n",
    "print(len(sen_list_20[i][i]),sen_list_20[i][i])   #processed first layer and first head of first sentence \n",
    "print(\"\\n Threshold 25\")\n",
    "print(len(sen_list[i][i]),sen_list[i][i])   #processed first layer and first head of first sentence \n",
    "print(\"\\n Threshold 30\")\n",
    "print(len(sen_list_30[i][i]),sen_list_30[i][i])   #processed first layer and first head of first sentence \n",
    "\n",
    "#print(len(sen_list[143][0]),sen_list[143][0]) #processed last layer and last head of first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 144/144 [10:52<00:00,  4.53s/it]\n",
      "100%|| 144/144 [10:56<00:00,  4.56s/it]\n",
      "100%|| 144/144 [10:47<00:00,  4.50s/it]\n"
     ]
    }
   ],
   "source": [
    "#scores = get_block_head(sen_list)\n",
    "#scores_10 = get_block_head(sen_list_10)\n",
    "scores_15 = get_block_head(sen_list_15)\n",
    "scores_20 = get_block_head(sen_list_20)\n",
    "scores_30 = get_block_head(sen_list_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 8, 0.20559701026283966), (8, 11, 0.19127006203151573), (8, 9, 0.18814623826875007), (11, 8, 0.18692809688409256), (7, 5, 0.18353356225877854), (9, 9, 0.18059253591562302), (7, 9, 0.17818772485683795), (6, 5, 0.1774358329795011), (9, 4, 0.17600542364521263), (9, 6, 0.17376864976801762)]\n",
      "[(8, 11, 0.17688744312860485), (8, 3, 0.16646452595741665), (8, 4, 0.16536730077420828), (8, 7, 0.16113962145781294), (8, 0, 0.15593628422752848), (8, 2, 0.15470411915568807), (9, 0, 0.15140050671408953), (7, 5, 0.15072199877638995), (8, 8, 0.14986440507830787), (8, 9, 0.14957368786096775)]\n"
     ]
    }
   ],
   "source": [
    "print(scores_10[0:10])  #10 thresh\n",
    "#  [(8, 11, 0.17688744312860485), (8, 3, 0.16646452595741665), (8, 4, 0.16536730077420828), (8, 7, 0.16113962145781294), (8, 0, 0.15593628422752848), (8, 2, 0.15470411915568807), (9, 0, 0.15140050671408953), (7, 5, 0.15072199877638995), (8, 8, 0.14986440507830787), (8, 9, 0.14957368786096775)]\n",
    "\n",
    "print(scores_15[0:10])  #15 thresh\n",
    "\n",
    "print(scores_20[0:10])  #20 thresh\n",
    "\n",
    "print(scores[0:10])  #25 thresh\n",
    "#  [(8, 8, 0.20559701026283966), (8, 11, 0.19127006203151573), (8, 9, 0.18814623826875007), (11, 8, 0.18692809688409256), (7, 5, 0.18353356225877854), (9, 9, 0.18059253591562302), (7, 9, 0.17818772485683795), (6, 5, 0.1774358329795011), (9, 4, 0.17600542364521263), (9, 6, 0.17376864976801762)]\n",
    "\n",
    "print(scores_30[0:10])  #30 thresh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1, 0.23903657674356474),\n",
       " (3, 2, 0.22334244144017074),\n",
       " (11, 5, 0.22290891025170304),\n",
       " (6, 0, 0.21972539246421388),\n",
       " (4, 2, 0.21848756769808247),\n",
       " (10, 11, 0.21640071453619267),\n",
       " (5, 4, 0.20899156231361693),\n",
       " (4, 1, 0.2067413901469147),\n",
       " (9, 7, 0.20085241894907047),\n",
       " (4, 9, 0.1969363758990871),\n",
       " (6, 5, 0.19436958617237984),\n",
       " (10, 0, 0.1941946590678377),\n",
       " (5, 7, 0.19313810872086903),\n",
       " (8, 4, 0.19215605317168943),\n",
       " (8, 8, 0.18770924721348628),\n",
       " (6, 8, 0.18629729823007607),\n",
       " (8, 9, 0.18483354081234135),\n",
       " (9, 8, 0.18213737216825984),\n",
       " (10, 10, 0.18066124945832143),\n",
       " (6, 2, 0.17955855985333163),\n",
       " (9, 5, 0.1782620783663914),\n",
       " (7, 5, 0.17782650826477636),\n",
       " (8, 5, 0.1750809159697408),\n",
       " (8, 6, 0.17116546321875653),\n",
       " (8, 0, 0.17101999169715307),\n",
       " (6, 7, 0.17075152030632992),\n",
       " (9, 10, 0.16983285936185574),\n",
       " (9, 3, 0.16933621140176697),\n",
       " (6, 4, 0.1656612731118689),\n",
       " (8, 11, 0.16559293441524148),\n",
       " (4, 11, 0.16244136493667177),\n",
       " (7, 0, 0.15778741111059338),\n",
       " (11, 2, 0.15771811491892582),\n",
       " (11, 9, 0.1541693440259734),\n",
       " (10, 4, 0.15182641855438994),\n",
       " (8, 10, 0.1494333893267871),\n",
       " (6, 6, 0.1486183062584987),\n",
       " (4, 6, 0.1485121350529015),\n",
       " (6, 11, 0.14616055669578518),\n",
       " (4, 4, 0.14595315745616766),\n",
       " (5, 3, 0.1452600371389312),\n",
       " (8, 2, 0.14207932213296157),\n",
       " (11, 0, 0.14124012884155004),\n",
       " (10, 6, 0.14113836559037662),\n",
       " (11, 11, 0.14067368798987784),\n",
       " (4, 7, 0.14035772913007338),\n",
       " (8, 3, 0.1401644358154404),\n",
       " (11, 7, 0.14006396820906786),\n",
       " (11, 1, 0.13950213094019337),\n",
       " (11, 3, 0.13851649567255905),\n",
       " (6, 1, 0.1370440473100292),\n",
       " (10, 2, 0.1366707844055332),\n",
       " (6, 10, 0.13609105266020555),\n",
       " (5, 2, 0.1358202019107529),\n",
       " (11, 8, 0.1349294833107715),\n",
       " (10, 1, 0.13485114328135256),\n",
       " (10, 7, 0.13469741679122532),\n",
       " (3, 6, 0.13424493865590464),\n",
       " (10, 3, 0.1340317730167978),\n",
       " (4, 3, 0.13366829219672652),\n",
       " (7, 9, 0.1327338832168681),\n",
       " (5, 8, 0.1320738977571235),\n",
       " (7, 4, 0.1318912914343732),\n",
       " (5, 5, 0.13175194669565946),\n",
       " (10, 8, 0.13168917493753318),\n",
       " (11, 4, 0.13045016237876791),\n",
       " (7, 2, 0.12974020398731256),\n",
       " (9, 0, 0.12820001400939154),\n",
       " (5, 10, 0.12779472037339562),\n",
       " (9, 2, 0.12630340980264781),\n",
       " (9, 4, 0.12614358037223602),\n",
       " (7, 10, 0.126004764279385),\n",
       " (7, 7, 0.1256395774033541),\n",
       " (5, 11, 0.1251331822420115),\n",
       " (9, 1, 0.1247822916093963),\n",
       " (2, 7, 0.12417690977862067),\n",
       " (7, 8, 0.12323587304796586),\n",
       " (10, 5, 0.12297736332054889),\n",
       " (9, 9, 0.12256917930857014),\n",
       " (5, 0, 0.12246831494896987),\n",
       " (3, 4, 0.12190025795034218),\n",
       " (7, 1, 0.12138096268780839),\n",
       " (0, 4, 0.1210144647472587),\n",
       " (3, 5, 0.12096072160621732),\n",
       " (2, 3, 0.12092496623638248),\n",
       " (11, 10, 0.12090430123780756),\n",
       " (1, 1, 0.12058078959421911),\n",
       " (0, 0, 0.11987742761091895),\n",
       " (9, 11, 0.11983251557584347),\n",
       " (7, 6, 0.11910646620921422),\n",
       " (4, 0, 0.11902995272997739),\n",
       " (3, 10, 0.11876368716113682),\n",
       " (2, 2, 0.1185443317630272),\n",
       " (4, 8, 0.11833797790837855),\n",
       " (6, 9, 0.11749092266515027),\n",
       " (3, 8, 0.11709475629864158),\n",
       " (1, 9, 0.11686831682050483),\n",
       " (2, 5, 0.11683294793499945),\n",
       " (8, 7, 0.11662213699303678),\n",
       " (7, 11, 0.1164310975125378),\n",
       " (0, 9, 0.11583271543459653),\n",
       " (7, 3, 0.11561447322148423),\n",
       " (5, 1, 0.11541527527890719),\n",
       " (6, 3, 0.11539918556801507),\n",
       " (3, 7, 0.11492908912959962),\n",
       " (2, 9, 0.11457794292701412),\n",
       " (4, 5, 0.11406652893591902),\n",
       " (0, 3, 0.11347259059401094),\n",
       " (1, 6, 0.11326190369935557),\n",
       " (10, 9, 0.1128458962146832),\n",
       " (4, 10, 0.1128009999274843),\n",
       " (1, 2, 0.11249556041494384),\n",
       " (3, 11, 0.11240006745644657),\n",
       " (3, 9, 0.11187086557314938),\n",
       " (0, 8, 0.11185795788603305),\n",
       " (2, 1, 0.11184227421854402),\n",
       " (5, 9, 0.1117949252098748),\n",
       " (0, 10, 0.11125839925549841),\n",
       " (2, 6, 0.11120353018078827),\n",
       " (0, 1, 0.11110620261502906),\n",
       " (1, 10, 0.11078926495242485),\n",
       " (2, 0, 0.11056452072223104),\n",
       " (1, 11, 0.11055677233685371),\n",
       " (0, 11, 0.11050952970741806),\n",
       " (3, 3, 0.11047849879055233),\n",
       " (0, 5, 0.11021734344122748),\n",
       " (3, 1, 0.11018946886356826),\n",
       " (2, 10, 0.10962099298939502),\n",
       " (0, 7, 0.10878052612959854),\n",
       " (3, 0, 0.10862221266936933),\n",
       " (1, 0, 0.10836133631894736),\n",
       " (2, 11, 0.10828972428341828),\n",
       " (1, 8, 0.10799836644179461),\n",
       " (0, 2, 0.10793480473266001),\n",
       " (9, 6, 0.10747256422945496),\n",
       " (1, 7, 0.10742426305743508),\n",
       " (2, 4, 0.10717004468036893),\n",
       " (1, 4, 0.10713829798493762),\n",
       " (5, 6, 0.10712936552946001),\n",
       " (1, 3, 0.10662285031537275),\n",
       " (2, 8, 0.10644658045626329),\n",
       " (1, 5, 0.10581179695692038),\n",
       " (11, 6, 0.10552712113280183),\n",
       " (0, 6, 0.10443320219533686)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for Yelp TDRG   <-- do to show its weakness   #I got layer 8 and head 1    ( but still attribution is pretty uniform)\n",
    "\n",
    "#scores   \n",
    "#[(8, 1, 0.23903657674356474), (3, 2, 0.22334244144017074), (11, 5, 0.22290891025170304), (6, 0, 0.21972539246421388), (4, 2, 0.21848756769808247), (10, 11, 0.21640071453619267), (5, 4, 0.20899156231361693), (4, 1, 0.2067413901469147), (9, 7, 0.20085241894907047), (4, 9, 0.1969363758990871), (6, 5, 0.19436958617237984), (10, 0, 0.1941946590678377), (5, 7, 0.19313810872086903), (8, 4, 0.19215605317168943), (8, 8, 0.18770924721348628), (6, 8, 0.18629729823007607), (8, 9, 0.18483354081234135), (9, 8, 0.18213737216825984), (10, 10, 0.18066124945832143), (6, 2, 0.17955855985333163), (9, 5, 0.1782620783663914), (7, 5, 0.17782650826477636), (8, 5, 0.1750809159697408), (8, 6, 0.17116546321875653), (8, 0, 0.17101999169715307), (6, 7, 0.17075152030632992), (9, 10, 0.16983285936185574), (9, 3, 0.16933621140176697), (6, 4, 0.1656612731118689), (8, 11, 0.16559293441524148), (4, 11, 0.16244136493667177), (7, 0, 0.15778741111059338), (11, 2, 0.15771811491892582), (11, 9, 0.1541693440259734), (10, 4, 0.15182641855438994), (8, 10, 0.1494333893267871), (6, 6, 0.1486183062584987), (4, 6, 0.1485121350529015), (6, 11, 0.14616055669578518), (4, 4, 0.14595315745616766), (5, 3, 0.1452600371389312), (8, 2, 0.14207932213296157), (11, 0, 0.14124012884155004), (10, 6, 0.14113836559037662), (11, 11, 0.14067368798987784), (4, 7, 0.14035772913007338), (8, 3, 0.1401644358154404), (11, 7, 0.14006396820906786), (11, 1, 0.13950213094019337), (11, 3, 0.13851649567255905), (6, 1, 0.1370440473100292), (10, 2, 0.1366707844055332), (6, 10, 0.13609105266020555), (5, 2, 0.1358202019107529), (11, 8, 0.1349294833107715), (10, 1, 0.13485114328135256), (10, 7, 0.13469741679122532), (3, 6, 0.13424493865590464), (10, 3, 0.1340317730167978), (4, 3, 0.13366829219672652), (7, 9, 0.1327338832168681), (5, 8, 0.1320738977571235), (7, 4, 0.1318912914343732), (5, 5, 0.13175194669565946), (10, 8, 0.13168917493753318), (11, 4, 0.13045016237876791), (7, 2, 0.12974020398731256), (9, 0, 0.12820001400939154), (5, 10, 0.12779472037339562), (9, 2, 0.12630340980264781), (9, 4, 0.12614358037223602), (7, 10, 0.126004764279385), (7, 7, 0.1256395774033541), (5, 11, 0.1251331822420115), (9, 1, 0.1247822916093963), (2, 7, 0.12417690977862067), (7, 8, 0.12323587304796586), (10, 5, 0.12297736332054889), (9, 9, 0.12256917930857014), (5, 0, 0.12246831494896987), (3, 4, 0.12190025795034218), (7, 1, 0.12138096268780839), (0, 4, 0.1210144647472587), (3, 5, 0.12096072160621732), (2, 3, 0.12092496623638248), (11, 10, 0.12090430123780756), (1, 1, 0.12058078959421911), (0, 0, 0.11987742761091895), (9, 11, 0.11983251557584347), (7, 6, 0.11910646620921422), (4, 0, 0.11902995272997739), (3, 10, 0.11876368716113682), (2, 2, 0.1185443317630272), (4, 8, 0.11833797790837855), (6, 9, 0.11749092266515027), (3, 8, 0.11709475629864158), (1, 9, 0.11686831682050483), (2, 5, 0.11683294793499945), (8, 7, 0.11662213699303678), (7, 11, 0.1164310975125378), (0, 9, 0.11583271543459653), (7, 3, 0.11561447322148423), (5, 1, 0.11541527527890719), (6, 3, 0.11539918556801507), (3, 7, 0.11492908912959962), (2, 9, 0.11457794292701412), (4, 5, 0.11406652893591902), (0, 3, 0.11347259059401094), (1, 6, 0.11326190369935557), (10, 9, 0.1128458962146832), (4, 10, 0.1128009999274843), (1, 2, 0.11249556041494384), (3, 11, 0.11240006745644657), (3, 9, 0.11187086557314938), (0, 8, 0.11185795788603305), (2, 1, 0.11184227421854402), (5, 9, 0.1117949252098748), (0, 10, 0.11125839925549841), (2, 6, 0.11120353018078827), (0, 1, 0.11110620261502906), (1, 10, 0.11078926495242485), (2, 0, 0.11056452072223104), (1, 11, 0.11055677233685371), (0, 11, 0.11050952970741806), (3, 3, 0.11047849879055233), (0, 5, 0.11021734344122748), (3, 1, 0.11018946886356826), (2, 10, 0.10962099298939502), (0, 7, 0.10878052612959854), (3, 0, 0.10862221266936933), (1, 0, 0.10836133631894736), (2, 11, 0.10828972428341828), (1, 8, 0.10799836644179461), (0, 2, 0.10793480473266001), (9, 6, 0.10747256422945496), (1, 7, 0.10742426305743508), (2, 4, 0.10717004468036893), (1, 4, 0.10713829798493762), (5, 6, 0.10712936552946001), (1, 3, 0.10662285031537275), (2, 8, 0.10644658045626329), (1, 5, 0.10581179695692038), (11, 6, 0.10552712113280183), (0, 6, 0.10443320219533686)]\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 0.15872124043746288)\n",
      "(8, 7, 0.15313005978987224)\n",
      "(8, 11, 0.151654679255837)\n",
      "(10, 4, 0.15127377859204544)\n",
      "(10, 9, 0.1511029737080658)\n",
      "(10, 11, 0.15107000490937236)\n",
      "(6, 2, 0.14955530722068505)\n",
      "(4, 1, 0.14928748884398232)\n",
      "(7, 0, 0.14906258472175327)\n",
      "(7, 11, 0.1475579435228811)\n",
      "(9, 7, 0.14500589475632758)\n",
      "(2, 2, 0.14450277782390422)\n",
      "(8, 8, 0.14346186616448636)\n",
      "(10, 1, 0.1426556151733003)\n",
      "(6, 1, 0.14251394857728616)\n",
      "(6, 0, 0.14246976886282084)\n",
      "(4, 2, 0.1424644795317995)\n",
      "(7, 10, 0.14204145834056617)\n",
      "(9, 5, 0.14183040851051623)\n",
      "(6, 7, 0.14145037448127487)\n",
      "(9, 10, 0.14062761350387756)\n",
      "(9, 2, 0.13944930105164943)\n",
      "(8, 10, 0.1394157092380312)\n",
      "(4, 0, 0.1393841124805446)\n",
      "(9, 0, 0.1393034407142085)\n",
      "(10, 5, 0.13876747507266696)\n",
      "(6, 10, 0.13792932115423973)\n",
      "(8, 6, 0.1378217878827729)\n",
      "(6, 11, 0.13779677020917627)\n",
      "(5, 1, 0.13614893997016472)\n",
      "(5, 8, 0.13605227816702284)\n",
      "(9, 8, 0.13602680569528341)\n",
      "(6, 4, 0.13581095497155543)\n",
      "(8, 3, 0.13579601390585622)\n",
      "(8, 0, 0.13576425923300012)\n",
      "(8, 9, 0.1351671716930027)\n",
      "(7, 9, 0.13496449550858283)\n",
      "(3, 2, 0.13484711935303764)\n",
      "(0, 3, 0.13481890472122618)\n",
      "(8, 2, 0.13464995774822877)\n",
      "(2, 5, 0.13447915228750212)\n",
      "(9, 3, 0.1344197740865365)\n",
      "(11, 8, 0.13435401549791162)\n",
      "(10, 10, 0.13350479585510758)\n",
      "(7, 5, 0.1333894303082348)\n",
      "(11, 9, 0.1333751885557493)\n",
      "(6, 5, 0.13301479766761298)\n",
      "(10, 8, 0.13280583317531783)\n",
      "(9, 6, 0.13246588369354667)\n",
      "(4, 6, 0.13243420122540095)\n",
      "(5, 10, 0.13220850602208492)\n",
      "(9, 1, 0.13217866792451535)\n",
      "(9, 4, 0.13206194531779925)\n",
      "(11, 3, 0.13180020830355996)\n",
      "(10, 7, 0.1317089754071112)\n",
      "(1, 6, 0.13144351612055208)\n",
      "(7, 1, 0.13089333295705807)\n",
      "(11, 4, 0.13039057194722992)\n",
      "(10, 6, 0.13005207231999905)\n",
      "(4, 9, 0.13002909604783539)\n",
      "(2, 7, 0.12981594326881582)\n",
      "(11, 10, 0.12972515254000253)\n",
      "(1, 2, 0.12927207793262022)\n",
      "(8, 5, 0.1292071897500579)\n",
      "(0, 9, 0.12888396426819398)\n",
      "(3, 7, 0.12852177124329545)\n",
      "(7, 6, 0.12808137945303547)\n",
      "(11, 0, 0.12803899718138)\n",
      "(10, 0, 0.12790238953622862)\n",
      "(10, 2, 0.12786305551814117)\n",
      "(2, 9, 0.12783227992315818)\n",
      "(3, 4, 0.12722450881329755)\n",
      "(11, 11, 0.12712391249817462)\n",
      "(7, 4, 0.1271169891985079)\n",
      "(4, 3, 0.12632819869419099)\n",
      "(5, 2, 0.12608901808392106)\n",
      "(0, 1, 0.12547062489074765)\n",
      "(4, 7, 0.12541943664395902)\n",
      "(6, 9, 0.1252608993241601)\n",
      "(6, 6, 0.12494634914607503)\n",
      "(5, 4, 0.12489168136215659)\n",
      "(5, 5, 0.12451143713016438)\n",
      "(11, 1, 0.12404935013024683)\n",
      "(3, 10, 0.12404142916804765)\n",
      "(1, 5, 0.1238771862325785)\n",
      "(2, 10, 0.12375788971487339)\n",
      "(7, 7, 0.12355560086699194)\n",
      "(0, 4, 0.12321506741984319)\n",
      "(11, 5, 0.12296320226291407)\n",
      "(0, 10, 0.12291335648297276)\n",
      "(3, 1, 0.12254841213048381)\n",
      "(5, 3, 0.12249976870858453)\n",
      "(0, 2, 0.12223461401217399)\n",
      "(5, 6, 0.12210618258402973)\n",
      "(3, 6, 0.1220572657786046)\n",
      "(6, 3, 0.1217105137811356)\n",
      "(2, 8, 0.12126000792615255)\n",
      "(0, 11, 0.12119611238277553)\n",
      "(1, 9, 0.12088452882965547)\n",
      "(9, 9, 0.1208527063507028)\n",
      "(1, 8, 0.12077363286966585)\n",
      "(9, 11, 0.12052156437737084)\n",
      "(5, 0, 0.12050654600642946)\n",
      "(1, 1, 0.120463997090209)\n",
      "(0, 7, 0.12040692805064161)\n",
      "(11, 7, 0.1201434972913248)\n",
      "(5, 9, 0.11990198081440019)\n",
      "(2, 0, 0.1197717409534933)\n",
      "(4, 5, 0.11945892787119682)\n",
      "(4, 4, 0.11918670184244103)\n",
      "(4, 8, 0.11914381404109033)\n",
      "(5, 7, 0.11911512128043117)\n",
      "(3, 11, 0.1190332451778208)\n",
      "(2, 3, 0.11890036721914864)\n",
      "(11, 2, 0.11826835750045142)\n",
      "(7, 3, 0.11800981196181094)\n",
      "(0, 0, 0.11758110808274784)\n",
      "(2, 11, 0.11745373030200242)\n",
      "(0, 5, 0.1168610577378327)\n",
      "(7, 2, 0.11674721540774494)\n",
      "(3, 9, 0.1162602910702793)\n",
      "(7, 8, 0.1162366744590177)\n",
      "(1, 10, 0.115982601655219)\n",
      "(1, 11, 0.11594933245397554)\n",
      "(3, 8, 0.11583938079406397)\n",
      "(2, 4, 0.11532137271748255)\n",
      "(6, 8, 0.11513349672694094)\n",
      "(3, 5, 0.11501390397261899)\n",
      "(1, 4, 0.11486056402293496)\n",
      "(1, 0, 0.11393373470821294)\n",
      "(3, 0, 0.11380881964216513)\n",
      "(11, 6, 0.11363841942829656)\n",
      "(4, 10, 0.11292269209909883)\n",
      "(0, 6, 0.11277873288732128)\n",
      "(1, 3, 0.11277684688170973)\n",
      "(2, 1, 0.11243845893059175)\n",
      "(8, 1, 0.11234064005650772)\n",
      "(0, 8, 0.1122476731796348)\n",
      "(3, 3, 0.11173446393479582)\n",
      "(1, 7, 0.1116831068377234)\n",
      "(2, 6, 0.1112762800197291)\n",
      "(10, 3, 0.11071787903662576)\n",
      "(5, 11, 0.10859594563838906)\n",
      "(4, 11, 0.10760757406017075)\n"
     ]
    }
   ],
   "source": [
    "# FOR IMAGE CAPTION\n",
    "for s in scores:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 5, 0.2821267870197217)\n",
      "(9, 11, 0.2804659528610392)\n",
      "(4, 1, 0.2702826391652081)\n",
      "(4, 2, 0.2686453297203014)\n",
      "(7, 5, 0.2558982372686172)\n",
      "(10, 1, 0.2548711599639146)\n",
      "(3, 6, 0.2547133216759923)\n",
      "(6, 4, 0.25457741595840155)\n",
      "(10, 6, 0.2540983151499425)\n",
      "(8, 10, 0.2530348236594129)\n",
      "(9, 0, 0.25153850235432024)\n",
      "(9, 7, 0.25086725930265563)\n",
      "(9, 8, 0.2497266541711812)\n",
      "(11, 11, 0.24868830116149063)\n",
      "(5, 2, 0.24625265297750812)\n",
      "(11, 0, 0.24587316106601706)\n",
      "(8, 1, 0.2458382475522489)\n",
      "(6, 7, 0.2434262283835387)\n",
      "(6, 0, 0.24253808384613174)\n",
      "(8, 2, 0.24121870636911416)\n",
      "(11, 7, 0.24070947092925374)\n",
      "(3, 4, 0.2403634657335317)\n",
      "(11, 5, 0.2393964068587743)\n",
      "(11, 3, 0.23590736514637933)\n",
      "(4, 6, 0.23542582390969882)\n",
      "(7, 4, 0.23472813429487474)\n",
      "(10, 8, 0.2344309689666082)\n",
      "(11, 1, 0.23331967464094852)\n",
      "(2, 5, 0.2328445136874893)\n",
      "(3, 2, 0.23235173020481859)\n",
      "(10, 7, 0.23229496479943998)\n",
      "(10, 2, 0.2318160123320329)\n",
      "(8, 4, 0.23155748020731803)\n",
      "(7, 6, 0.23106784687559856)\n",
      "(10, 11, 0.22818983501086143)\n",
      "(8, 11, 0.22798889215875043)\n",
      "(8, 3, 0.22721118692004794)\n",
      "(9, 10, 0.22672459120209093)\n",
      "(3, 10, 0.22519058000758194)\n",
      "(8, 9, 0.2241805818669588)\n",
      "(9, 9, 0.22405044368426374)\n",
      "(10, 3, 0.223555245424775)\n",
      "(8, 7, 0.22317801284393465)\n",
      "(10, 5, 0.22312233036700804)\n",
      "(8, 0, 0.2228664420502504)\n",
      "(4, 4, 0.22169891431995845)\n",
      "(7, 10, 0.2212123765548484)\n",
      "(9, 2, 0.22093261854156906)\n",
      "(5, 8, 0.22021984599267988)\n",
      "(0, 4, 0.21998517298953668)\n",
      "(11, 4, 0.219884022768785)\n",
      "(7, 9, 0.21932845882741908)\n",
      "(8, 6, 0.21924857473826811)\n",
      "(5, 10, 0.2189380112693626)\n",
      "(7, 0, 0.21815320145117337)\n",
      "(11, 9, 0.2177186846156503)\n",
      "(10, 9, 0.21762463074705302)\n",
      "(9, 3, 0.21747216627504515)\n",
      "(4, 9, 0.21709603802286437)\n",
      "(6, 5, 0.21628634689262732)\n",
      "(10, 4, 0.21612818836283515)\n",
      "(5, 4, 0.2156292847070096)\n",
      "(11, 8, 0.21556118520506434)\n",
      "(4, 0, 0.21442900269640897)\n",
      "(4, 10, 0.2142588172043265)\n",
      "(9, 1, 0.21401057124101527)\n",
      "(5, 3, 0.21390567942065053)\n",
      "(1, 9, 0.21382035426269344)\n",
      "(5, 0, 0.2135891014406193)\n",
      "(2, 2, 0.21357474388540695)\n",
      "(4, 11, 0.21323404911739913)\n",
      "(4, 7, 0.21270422969551242)\n",
      "(6, 10, 0.2114882633562771)\n",
      "(1, 1, 0.21131652206125864)\n",
      "(6, 1, 0.21070345167153623)\n",
      "(8, 8, 0.20853575478811975)\n",
      "(0, 10, 0.20825084057112414)\n",
      "(5, 9, 0.20815130957729655)\n",
      "(3, 8, 0.2080893487712242)\n",
      "(7, 1, 0.20798775618478088)\n",
      "(7, 7, 0.20796259011088877)\n",
      "(8, 5, 0.20793326920713062)\n",
      "(10, 10, 0.20758478951488427)\n",
      "(9, 6, 0.20706231087806054)\n",
      "(5, 5, 0.20668205235209797)\n",
      "(5, 7, 0.2064929613706191)\n",
      "(3, 7, 0.2059034465378773)\n",
      "(6, 9, 0.20570495221700738)\n",
      "(3, 1, 0.20444096105252693)\n",
      "(7, 8, 0.2044262730577724)\n",
      "(2, 7, 0.20323437666776947)\n",
      "(2, 11, 0.2030539526217275)\n",
      "(4, 5, 0.20296493557282066)\n",
      "(0, 8, 0.20275966941225015)\n",
      "(7, 11, 0.20131804916248006)\n",
      "(5, 6, 0.2013148670054958)\n",
      "(6, 6, 0.20067911898677848)\n",
      "(1, 8, 0.20009451206287365)\n",
      "(4, 3, 0.19960865075041898)\n",
      "(5, 1, 0.19910825773192797)\n",
      "(0, 11, 0.19881231455405726)\n",
      "(2, 0, 0.19877029896497667)\n",
      "(0, 1, 0.19876918677071598)\n",
      "(6, 2, 0.19795489534074362)\n",
      "(7, 3, 0.1969964603186869)\n",
      "(5, 11, 0.19574559938449518)\n",
      "(3, 9, 0.1954312920594976)\n",
      "(2, 3, 0.19476243174443364)\n",
      "(1, 0, 0.19466715556241646)\n",
      "(11, 10, 0.1943837042616876)\n",
      "(10, 0, 0.19431259999030978)\n",
      "(1, 4, 0.1940366451242541)\n",
      "(3, 11, 0.19350994381055336)\n",
      "(1, 5, 0.19326412370183527)\n",
      "(6, 3, 0.1928441913369266)\n",
      "(0, 3, 0.19283352297721543)\n",
      "(6, 8, 0.19217251699505788)\n",
      "(9, 4, 0.19189806026803832)\n",
      "(2, 9, 0.1912298840586314)\n",
      "(11, 6, 0.1906833788575826)\n",
      "(2, 4, 0.18965080300677695)\n",
      "(11, 2, 0.18942910988194592)\n",
      "(4, 8, 0.1893129560232815)\n",
      "(6, 11, 0.18880175657216688)\n",
      "(3, 3, 0.1884226442982149)\n",
      "(1, 6, 0.18789621819008862)\n",
      "(2, 1, 0.18785309183186294)\n",
      "(0, 0, 0.18743361806539105)\n",
      "(1, 2, 0.1850037670403799)\n",
      "(7, 2, 0.18448935724677665)\n",
      "(1, 3, 0.18333391786093758)\n",
      "(2, 8, 0.18235545615175539)\n",
      "(0, 9, 0.18230864002459135)\n",
      "(0, 7, 0.18215856710272174)\n",
      "(1, 10, 0.18200776772071642)\n",
      "(1, 7, 0.181427734607303)\n",
      "(2, 10, 0.18005576482176047)\n",
      "(3, 5, 0.1778054086826477)\n",
      "(0, 6, 0.17689931925350524)\n",
      "(3, 0, 0.17684329834686374)\n",
      "(2, 6, 0.1731044943310738)\n",
      "(1, 11, 0.172604894032888)\n",
      "(0, 2, 0.1709036210206209)\n",
      "(0, 5, 0.17061852198931773)\n"
     ]
    }
   ],
   "source": [
    "# FOR AMAZON  \n",
    "for s in scores:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5, 0.22910906945266027),\n",
       " (9, 4, 0.20670482156851616),\n",
       " (11, 5, 0.20384664328140217),\n",
       " (9, 9, 0.2027791205407322),\n",
       " (8, 4, 0.19366057264682351),\n",
       " (8, 3, 0.1929708452634522),\n",
       " (10, 11, 0.1924844757337584),\n",
       " (8, 0, 0.1912139770748914),\n",
       " (10, 8, 0.19096766338766102),\n",
       " (8, 1, 0.19064305221106376),\n",
       " (11, 7, 0.18917525337332117),\n",
       " (11, 0, 0.18796913047231342),\n",
       " (10, 2, 0.1857773846771663),\n",
       " (8, 8, 0.18555147862865973),\n",
       " (10, 0, 0.18530059177637234),\n",
       " (11, 2, 0.18495542770861484),\n",
       " (9, 2, 0.18475037539261013),\n",
       " (9, 0, 0.1841438644026269),\n",
       " (11, 4, 0.18140669805587734),\n",
       " (10, 9, 0.18016958380296674),\n",
       " (9, 1, 0.1799556444681705),\n",
       " (8, 11, 0.1797203777121738),\n",
       " (10, 5, 0.17958857143178505),\n",
       " (8, 7, 0.17886423133516335),\n",
       " (11, 3, 0.17881788652643082),\n",
       " (10, 4, 0.17843035955545758),\n",
       " (10, 6, 0.17813508280376536),\n",
       " (11, 6, 0.17802176577302084),\n",
       " (9, 10, 0.17787153222013147),\n",
       " (8, 5, 0.17732895140520186),\n",
       " (9, 3, 0.17613587979399356),\n",
       " (11, 1, 0.17594829998328604),\n",
       " (10, 10, 0.17578660223763148),\n",
       " (8, 9, 0.17564796681179257),\n",
       " (9, 7, 0.17563120922211783),\n",
       " (8, 2, 0.1753101101461102),\n",
       " (8, 10, 0.1747312199274393),\n",
       " (7, 4, 0.16945213119226252),\n",
       " (5, 7, 0.16936174418069228),\n",
       " (6, 10, 0.16795158412774336),\n",
       " (7, 6, 0.16649419325832185),\n",
       " (5, 4, 0.1651518529581891),\n",
       " (7, 9, 0.16469694778258695),\n",
       " (7, 3, 0.16452084200970687),\n",
       " (9, 6, 0.16354265678158442),\n",
       " (10, 3, 0.16331819279395884),\n",
       " (8, 6, 0.16331625466434685),\n",
       " (9, 8, 0.16327127028055513),\n",
       " (10, 7, 0.163098795384727),\n",
       " (7, 2, 0.1626278604999319),\n",
       " (11, 10, 0.16233153977410752),\n",
       " (4, 6, 0.16214737843926777),\n",
       " (11, 11, 0.16211695020543535),\n",
       " (7, 0, 0.16089281622059912),\n",
       " (7, 7, 0.16081335698132243),\n",
       " (7, 8, 0.1589754294115524),\n",
       " (9, 11, 0.1589265489047186),\n",
       " (11, 8, 0.15852928487757295),\n",
       " (7, 10, 0.15707820738638756),\n",
       " (5, 1, 0.15687621447472652),\n",
       " (10, 1, 0.15515437741163898),\n",
       " (2, 3, 0.1551460585797901),\n",
       " (6, 0, 0.1548807110553279),\n",
       " (7, 1, 0.15395799252569156),\n",
       " (7, 5, 0.15229415791014433),\n",
       " (4, 2, 0.15186431742637152),\n",
       " (5, 0, 0.15157253037355953),\n",
       " (6, 5, 0.15076005645689922),\n",
       " (6, 7, 0.15074430937346742),\n",
       " (5, 10, 0.15010913817566668),\n",
       " (5, 11, 0.15007268340070756),\n",
       " (3, 8, 0.15007106649191967),\n",
       " (5, 8, 0.14975564791320012),\n",
       " (4, 4, 0.1490504683894608),\n",
       " (5, 5, 0.149014187066919),\n",
       " (2, 5, 0.14814437405718062),\n",
       " (5, 3, 0.1477559440078062),\n",
       " (4, 3, 0.14737676824214005),\n",
       " (6, 2, 0.14630223368638817),\n",
       " (6, 4, 0.14562702254610183),\n",
       " (4, 0, 0.14541923635179313),\n",
       " (2, 11, 0.14486951594498368),\n",
       " (11, 9, 0.14390854882222753),\n",
       " (2, 2, 0.14186954191469386),\n",
       " (6, 8, 0.1415233051499205),\n",
       " (3, 6, 0.1412151891662137),\n",
       " (6, 11, 0.13976853577873255),\n",
       " (6, 9, 0.13964447988182316),\n",
       " (1, 2, 0.13864583210237966),\n",
       " (5, 2, 0.1377782747208492),\n",
       " (2, 10, 0.13716651926941828),\n",
       " (6, 1, 0.13680782823896354),\n",
       " (7, 11, 0.13541384084776464),\n",
       " (5, 6, 0.1332300926002088),\n",
       " (4, 11, 0.13282151345939952),\n",
       " (6, 6, 0.13196132172407055),\n",
       " (4, 8, 0.1315829140990315),\n",
       " (3, 10, 0.13112511141269376),\n",
       " (1, 7, 0.13011743578817267),\n",
       " (6, 3, 0.12996468854288415),\n",
       " (5, 9, 0.12973969064601398),\n",
       " (2, 7, 0.12912978677195777),\n",
       " (0, 4, 0.12901492519384994),\n",
       " (0, 8, 0.12812594403523836),\n",
       " (3, 2, 0.12743444680710783),\n",
       " (1, 9, 0.12738812246961215),\n",
       " (1, 6, 0.12680959161172786),\n",
       " (4, 1, 0.125215670769727),\n",
       " (4, 5, 0.12498888481834258),\n",
       " (2, 6, 0.12449183497329691),\n",
       " (0, 9, 0.12325136333246975),\n",
       " (0, 6, 0.12264510875982099),\n",
       " (1, 11, 0.1223869126832301),\n",
       " (4, 9, 0.12207874539677677),\n",
       " (4, 10, 0.12204815966692621),\n",
       " (4, 7, 0.12167764101560813),\n",
       " (0, 5, 0.1216509180446421),\n",
       " (2, 1, 0.12154367079065047),\n",
       " (0, 10, 0.12070691094214922),\n",
       " (3, 1, 0.12063199760152898),\n",
       " (1, 8, 0.11968806324305246),\n",
       " (3, 7, 0.1195478440793774),\n",
       " (3, 4, 0.11938728747427772),\n",
       " (1, 1, 0.11895614866267316),\n",
       " (0, 7, 0.11866641294412643),\n",
       " (2, 8, 0.11832455905426274),\n",
       " (1, 5, 0.11776213346842508),\n",
       " (0, 3, 0.11548711856593791),\n",
       " (1, 3, 0.11514066073132391),\n",
       " (0, 11, 0.11501069082440221),\n",
       " (3, 3, 0.11458264421366839),\n",
       " (2, 4, 0.11449397341419225),\n",
       " (0, 0, 0.11299689002266738),\n",
       " (3, 9, 0.11285334549500341),\n",
       " (2, 0, 0.11282576791440299),\n",
       " (3, 5, 0.1126289646281459),\n",
       " (2, 9, 0.11251620759882218),\n",
       " (1, 4, 0.1122227909962318),\n",
       " (1, 10, 0.11147971908048575),\n",
       " (0, 2, 0.11145458944377395),\n",
       " (1, 0, 0.11133206961487419),\n",
       " (0, 1, 0.10845245556169843),\n",
       " (3, 11, 0.10791545942834053),\n",
       " (3, 0, 0.10664399820341712)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for LIPTON  .. this took around 11 minutes using ./data/lipton/sentiment/orig/bert_classifier_10epochs8b_490seqlen/\n",
    "#scores    #according to this layer 9 and head 5 gives the most but its pretty uniform .. hmmm.. not super convincing ( why not an ensemble then ? )\n",
    "\n",
    "#[(9, 5, 0.22910906945266027), (9, 4, 0.20670482156851616), (11, 5, 0.20384664328140217), (9, 9, 0.2027791205407322), (8, 4, 0.19366057264682351), (8, 3, 0.1929708452634522), (10, 11, 0.1924844757337584), (8, 0, 0.1912139770748914), (10, 8, 0.19096766338766102), (8, 1, 0.19064305221106376), (11, 7, 0.18917525337332117), (11, 0, 0.18796913047231342), (10, 2, 0.1857773846771663), (8, 8, 0.18555147862865973), (10, 0, 0.18530059177637234), (11, 2, 0.18495542770861484), (9, 2, 0.18475037539261013), (9, 0, 0.1841438644026269), (11, 4, 0.18140669805587734), (10, 9, 0.18016958380296674), (9, 1, 0.1799556444681705), (8, 11, 0.1797203777121738), (10, 5, 0.17958857143178505), (8, 7, 0.17886423133516335), (11, 3, 0.17881788652643082), (10, 4, 0.17843035955545758), (10, 6, 0.17813508280376536), (11, 6, 0.17802176577302084), (9, 10, 0.17787153222013147), (8, 5, 0.17732895140520186), (9, 3, 0.17613587979399356), (11, 1, 0.17594829998328604), (10, 10, 0.17578660223763148), (8, 9, 0.17564796681179257), (9, 7, 0.17563120922211783), (8, 2, 0.1753101101461102), (8, 10, 0.1747312199274393), (7, 4, 0.16945213119226252), (5, 7, 0.16936174418069228), (6, 10, 0.16795158412774336), (7, 6, 0.16649419325832185), (5, 4, 0.1651518529581891), (7, 9, 0.16469694778258695), (7, 3, 0.16452084200970687), (9, 6, 0.16354265678158442), (10, 3, 0.16331819279395884), (8, 6, 0.16331625466434685), (9, 8, 0.16327127028055513), (10, 7, 0.163098795384727), (7, 2, 0.1626278604999319), (11, 10, 0.16233153977410752), (4, 6, 0.16214737843926777), (11, 11, 0.16211695020543535), (7, 0, 0.16089281622059912), (7, 7, 0.16081335698132243), (7, 8, 0.1589754294115524), (9, 11, 0.1589265489047186), (11, 8, 0.15852928487757295), (7, 10, 0.15707820738638756), (5, 1, 0.15687621447472652), (10, 1, 0.15515437741163898), (2, 3, 0.1551460585797901), (6, 0, 0.1548807110553279), (7, 1, 0.15395799252569156), (7, 5, 0.15229415791014433), (4, 2, 0.15186431742637152), (5, 0, 0.15157253037355953), (6, 5, 0.15076005645689922), (6, 7, 0.15074430937346742), (5, 10, 0.15010913817566668), (5, 11, 0.15007268340070756), (3, 8, 0.15007106649191967), (5, 8, 0.14975564791320012), (4, 4, 0.1490504683894608), (5, 5, 0.149014187066919), (2, 5, 0.14814437405718062), (5, 3, 0.1477559440078062), (4, 3, 0.14737676824214005), (6, 2, 0.14630223368638817), (6, 4, 0.14562702254610183), (4, 0, 0.14541923635179313), (2, 11, 0.14486951594498368), (11, 9, 0.14390854882222753), (2, 2, 0.14186954191469386), (6, 8, 0.1415233051499205), (3, 6, 0.1412151891662137), (6, 11, 0.13976853577873255), (6, 9, 0.13964447988182316), (1, 2, 0.13864583210237966), (5, 2, 0.1377782747208492), (2, 10, 0.13716651926941828), (6, 1, 0.13680782823896354), (7, 11, 0.13541384084776464), (5, 6, 0.1332300926002088), (4, 11, 0.13282151345939952), (6, 6, 0.13196132172407055), (4, 8, 0.1315829140990315), (3, 10, 0.13112511141269376), (1, 7, 0.13011743578817267), (6, 3, 0.12996468854288415), (5, 9, 0.12973969064601398), (2, 7, 0.12912978677195777), (0, 4, 0.12901492519384994), (0, 8, 0.12812594403523836), (3, 2, 0.12743444680710783), (1, 9, 0.12738812246961215), (1, 6, 0.12680959161172786), (4, 1, 0.125215670769727), (4, 5, 0.12498888481834258), (2, 6, 0.12449183497329691), (0, 9, 0.12325136333246975), (0, 6, 0.12264510875982099), (1, 11, 0.1223869126832301), (4, 9, 0.12207874539677677), (4, 10, 0.12204815966692621), (4, 7, 0.12167764101560813), (0, 5, 0.1216509180446421), (2, 1, 0.12154367079065047), (0, 10, 0.12070691094214922), (3, 1, 0.12063199760152898), (1, 8, 0.11968806324305246), (3, 7, 0.1195478440793774), (3, 4, 0.11938728747427772), (1, 1, 0.11895614866267316), (0, 7, 0.11866641294412643), (2, 8, 0.11832455905426274), (1, 5, 0.11776213346842508), (0, 3, 0.11548711856593791), (1, 3, 0.11514066073132391), (0, 11, 0.11501069082440221), (3, 3, 0.11458264421366839), (2, 4, 0.11449397341419225), (0, 0, 0.11299689002266738), (3, 9, 0.11285334549500341), (2, 0, 0.11282576791440299), (3, 5, 0.1126289646281459), (2, 9, 0.11251620759882218), (1, 4, 0.1122227909962318), (1, 10, 0.11147971908048575), (0, 2, 0.11145458944377395), (1, 0, 0.11133206961487419), (0, 1, 0.10845245556169843), (3, 11, 0.10791545942834053), (3, 0, 0.10664399820341712)]\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO integradted gradients / expected gradients / integrated hessians\n",
    "\n",
    "# IG using captum\n",
    "# pip install captum\n",
    "# https://captum.ai/tutorials/IMDB_TorchText_Interpret  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
